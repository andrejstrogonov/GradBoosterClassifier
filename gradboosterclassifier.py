# -*- coding: utf-8 -*-
"""GradBoosterClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UF4dg2gomZRJsRmwixkKitSKWOhfgjn9
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler as stsc
from sklearn.model_selection import train_test_split as tts
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
 
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix, classification_report,\
roc_auc_score, accuracy_score
from sklearn.metrics import make_scorer, matthews_corrcoef

from sklearn.preprocessing import StandardScaler

from time import time
from __future__ import division
import warnings
warnings.filterwarnings("ignore")

SEED = 7  # random state

url = "http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data"
secom = pd.read_table(url, header=None, delim_whitespace=True)
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data"
y = pd.read_table(url, header=None, usecols=[0], squeeze=True, delim_whitespace=True)
print('The dataset has {} observations/rows and {} variables/columns.' \
       .format(secom.shape[0], secom.shape[1]))
print('The ratio of majority class to minority class is {}:1.' \
      .format(int(y[y == -1].size/y[y == 1].size)))

# очистка данных
nmv = secom.dropna(axis=1)
print ('No. of columns after removing columns with missing data: {}'\
.format(nmv.shape[1])) 
nmv = secom.dropna(axis=0)
print('No. of rows after removing rows with missing data: {}'\
.format(nmv.shape[0]))
m = list(map(lambda x: sum(secom[x].isnull()), range(secom.shape[1])))
plt.title("Distribution of missing values")
plt.xlabel("No. of missing values in a column")
plt.ylabel("Columns")
plt.show()
plt.hist(m)

m_700thresh = list(filter(lambda i: (m[i] > 700), range(secom.shape[1])))
print('The number of columns with more than 700 missing values: {}'\
.format(len(m_700thresh)))
m_200thresh = list(filter(lambda i: (m[i] > 200), range(secom.shape[1])))
print('The number of columns with more than 200 missing values: {}'\
.format(len(m_200thresh)))

secom_drop_200thresh = secom.dropna(subset=m_200thresh, axis=1)
print('No. of columns after dropping columns with more than 200 missing entries: {}'\
.format(secom_drop_200thresh.shape[1]))

dropthese = [x for x in secom_drop_200thresh.columns.values \
             if secom_drop_200thresh[x].std() == 0]

print ('There are {} columns which have identical values recorded. \
We will drop these.' .format(len(dropthese)))

secom_drop_200thresh.drop(dropthese, axis=1, inplace=True)
print ('The SECOM data set now has {} variables.'\
.format(secom_drop_200thresh.shape[1]))

print('The number of categorical variables is: {}'\
.format(sum((secom_drop_200thresh.dtypes == 'categorical')*1))) 
secom_drop_200thresh.head(2)

secom = secom_drop_200thresh.fillna(0)
secomp_imp = StandardScaler().fit_transform(secom)

X_train, X_test, y_train, y_test = tts(secomp_imp, y, \
                                       test_size=0.2, stratify=y, random_state=5)

# function to test GBC parameters

def GBC(params, weight):
    
    clf = GradientBoostingClassifier(**params)
                            
    if weight:
        sample_weight = np.array([14 if i == -1 else 1 for i in y_train])
        clf.fit(X_train, y_train, sample_weight)
    else:
        clf.fit(X_train, y_train)
    print_results(clf, X_train, X_test)

    
# function to print results
def print_results(clf, X_train, X_test):        
    # training data results  
    print('\nTraining set results:') 
    y_pred = clf.predict(X_train)
    print ('The train set MCC: {0:4.3f}'\
    .format(matthews_corrcoef(y_train, y_pred)))
    
    # test set results
    print('\nTraining set results:') 
    acc = clf.score(X_test, y_test)
    print("Accuracy: {:.4f}".format(acc))
                             
    y_pred = clf.predict(X_test)
    print ('\nThe confusion matrix: ') 
    cm = confusion_matrix(y_test, y_pred)
    print (cm)
    print ('\nThe test set MCC: {0:4.3f}'\
    .format(matthews_corrcoef(y_test, y_pred)))

params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt',
          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}

GBC(params, 0)

params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt', 
          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}
GBC(params, 1)

params = {'n_estimators': 800, 'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt', 
          'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}

# GBM
clf = GradientBoostingClassifier(**params)
clf.fit(X_train, y_train)
gbm_importance = clf.feature_importances_
gbm_ranked_indices = np.argsort(clf.feature_importances_)[::-1]

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=7)
rf.fit(X_train, y_train)
rf_importance = rf.feature_importances_
rf_ranked_indices = np.argsort(rf.feature_importances_)[::-1]

# printing results in a table
importance_results = pd.DataFrame(index=range(1,16), 
                                  columns=pd.MultiIndex.from_product([['GBM','RF'],['Feature #','Importance']]))
importance_results.index.name = 'Rank'
importance_results.loc[:,'GBM'] =  list(zip(gbm_ranked_indices[:15], 
                                            gbm_importance[gbm_ranked_indices[:15]]))
importance_results.loc[:,'RF'] =  list(zip(rf_ranked_indices[:15], 
                                           rf_importance[rf_ranked_indices[:15]]))
print (importance_results)

# function to compute MCC vs number of trees

def GBC_trend(weight):
    base_params = {'max_depth': 3, 'subsample': 0.8, 'max_features' : 'sqrt',
                   'learning_rate': 0.019, 'min_samples_split': 2, 'random_state': SEED}

    mcc_train = []
    mcc_test = []

    for i in range(500, 1600, 100):
        params = dict(base_params)
        ntrees = {'n_estimators': i}
        params.update(ntrees)
    
        clf = GradientBoostingClassifier(**params)
        if weight:
            sample_weight = np.array([14 if i == -1 else 1 for i in y_train])
            clf.fit(X_train, y_train, sample_weight)
        else:
            clf.fit(X_train, y_train)
    
        y_pred_train = clf.predict(X_train)
        mcc_train.append(matthews_corrcoef(y_train, y_pred_train))
        y_pred_test = clf.predict(X_test)
        mcc_test.append(matthews_corrcoef(y_test, y_pred_test))
    
    return mcc_train, mcc_test

fig, ax = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(7,4))
plt.style.use('ggplot')

mcc_train, mcc_test = GBC_trend(0)
ax[0].plot(range(500, 1600, 100), mcc_train, color='magenta', label='train MCC' )
ax[0].plot(range(500, 1600, 100), mcc_test, color='olive', label='test MCC' )
ax[0].set_title('For default weights')

mcc_train, mcc_test = GBC_trend(1)
ax[1].plot(range(500, 1600, 100), mcc_train, color='magenta', label='train MCC' )
ax[1].plot(range(500, 1600, 100), mcc_test, color='olive', label='test MCC' )
ax[1].set_title('For sample weights')

ax[0].set_ylabel('MCC')
fig.text(0.5, 0.04, 'Boosting Iterations', ha='center', va='center')
plt.xlim(500,1500)
plt.ylim(-0.1, 1.1);
plt.legend(bbox_to_anchor=(1.05, 0), loc='lower left');